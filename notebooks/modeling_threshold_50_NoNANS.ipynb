{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c3112f-6e26-42b7-9e00-9ce2131dc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from matplotlib.colors import Colormap\n",
    "import scipy.stats as stats\n",
    "from numpy import interp\n",
    "import scikitplot as skplt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV, \n",
    "    cross_val_score, \n",
    "    cross_val_predict\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    r2_score,\n",
    "    mean_squared_error, \n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error, \n",
    "    mean_absolute_percentage_error,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    brier_score_loss,\n",
    "    f1_score,\n",
    "    roc_curve, \n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728bffd-8a2d-40a3-b4bc-9394221ca02c",
   "metadata": {},
   "source": [
    "### Import DataFrame from Prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21e0cd9-9a00-4afd-8174-40afb3d2b739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (349256, 39)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/data_threshed50.csv', low_memory=False)\n",
    "df = df.dropna()\n",
    "print(F\"Dataframe shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad9636-3944-43f5-a065-1564b28c8d87",
   "metadata": {},
   "source": [
    "### Converting Column Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0986d-395e-4c6c-ab80-565425b302b0",
   "metadata": {},
   "source": [
    "#### Object Columns to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab92989-4c10-4345-9b81-7c653a57dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns_to_change_to_category = [        \n",
    "    'eventDescription',  \n",
    "    'ecuSoftwareVersion',          \n",
    "    'ecuSerialNumber',         \n",
    "    'ecuModel',           \n",
    "    'ecuMake',                  \n",
    "    'EquipmentID',                                 \n",
    "    'LampStatus',\n",
    "    'next_derate_timestamp',\n",
    "    'time_until_derate'] \n",
    "\n",
    "for column in object_columns_to_change_to_category:\n",
    "    df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1bbeab-07eb-42d3-b0bf-7c582cefc552",
   "metadata": {},
   "source": [
    "#### INT Columns to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfc84b9-1a98-491d-a9a4-6bf80b0453e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns_to_categorical = ['RecordID',\n",
    "'ESS_Id',                    \n",
    "'ecuSource',                    \n",
    "'spn',                       \n",
    "'fmi',  \n",
    "'active',       \n",
    "'MCTNumber',                  \n",
    "'FaultId']\n",
    "for column in int_columns_to_categorical:\n",
    "    df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ad387-a89f-4c1a-8773-739c716d826f",
   "metadata": {},
   "source": [
    "#### Date Columns to DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b153fc-d380-4b13-bed1-0e9ae1aa5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EventTimeStamp'] = pd.to_datetime(df['EventTimeStamp'])\n",
    "df['LocationTimeStamp'] = pd.to_datetime(df['LocationTimeStamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed3c6b-f3eb-40a6-9327-8293a6e025c0",
   "metadata": {},
   "source": [
    "### Splitting Data Train/Test before and After 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eae9061-316d-4ac7-bd55-99a990479acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date = '2019-01-01'\n",
    "df_train = df.sort_values('EventTimeStamp').loc[df['EventTimeStamp'] < test_date]\n",
    "df_test = df.sort_values('EventTimeStamp').loc[df['EventTimeStamp'] > test_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb9ff27-f193-4e5d-a873-d2c5669394a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['target',\n",
    "            'LocationTimeStamp',\n",
    "            'EventTimeStamp',\n",
    "            'eventDescription',\n",
    "            'ESS_Id',\n",
    "            'ecuModel',\n",
    "            'ecuMake',\n",
    "            'ecuSource',\n",
    "            'ecuSoftwareVersion',\n",
    "            'ecuSerialNumber',\n",
    "            'MCTNumber',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "            'RecordID',\n",
    "            'next_derate_timestamp',\n",
    "            'time_until_derate',\n",
    "            'FaultId',\n",
    "            'EngineLoad',\n",
    "            'TurboBoostPressure',\n",
    "            'DistanceLtd',\n",
    "             'LampStatus',\n",
    "            'Unnamed: 0'], axis=1)\n",
    "\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "X_test = df_test.drop(['target',\n",
    "            'LocationTimeStamp',\n",
    "            'EventTimeStamp',\n",
    "            'eventDescription',\n",
    "            'ESS_Id',\n",
    "            'ecuModel',\n",
    "            'ecuMake',\n",
    "            'ecuSource',\n",
    "            'ecuSoftwareVersion',\n",
    "            'ecuSerialNumber',\n",
    "            'MCTNumber',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "            'RecordID',\n",
    "            'next_derate_timestamp',\n",
    "            'time_until_derate',\n",
    "            'FaultId',\n",
    "            'EngineLoad',\n",
    "            'TurboBoostPressure',\n",
    "            'DistanceLtd',\n",
    "             'LampStatus',\n",
    "             'Unnamed: 0'], axis=1)\n",
    "\n",
    "y_test = df_test['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11e82331-05f2-4d57-81b4-4377b6f110eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 327884 entries, 17 to 945556\n",
      "Data columns (total 17 columns):\n",
      " #   Column                     Non-Null Count   Dtype   \n",
      "---  ------                     --------------   -----   \n",
      " 0   spn                        327884 non-null  category\n",
      " 1   fmi                        327884 non-null  category\n",
      " 2   active                     327884 non-null  category\n",
      " 3   activeTransitionCount      327884 non-null  int64   \n",
      " 4   EquipmentID                327884 non-null  category\n",
      " 5   BarometricPressure         327884 non-null  float64 \n",
      " 6   CruiseControlActive        327884 non-null  bool    \n",
      " 7   EngineCoolantTemperature   327884 non-null  float64 \n",
      " 8   EngineOilPressure          327884 non-null  float64 \n",
      " 9   EngineOilTemperature       327884 non-null  float64 \n",
      " 10  EngineRpm                  327884 non-null  float64 \n",
      " 11  FuelLtd                    327884 non-null  float64 \n",
      " 12  FuelRate                   327884 non-null  float64 \n",
      " 13  IgnStatus                  327884 non-null  bool    \n",
      " 14  IntakeManifoldTemperature  327884 non-null  float64 \n",
      " 15  ParkingBrake               327884 non-null  bool    \n",
      " 16  Speed                      327884 non-null  float64 \n",
      "dtypes: bool(3), category(4), float64(9), int64(1)\n",
      "memory usage: 30.4 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c55a1-e650-45b7-8fc3-776385e6af7e",
   "metadata": {},
   "source": [
    "### Evaluating What Rows were Dropped in Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a745146e-c2de-407d-88ca-1ab62b9c5faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of rows in the df (349256, 39) The amount of rows in Training Df (327884, 17) The amount of rows in Testing Df (21372, 17)\n"
     ]
    }
   ],
   "source": [
    "print(F\"The amount of rows in the df {df.shape} The amount of rows in Training Df {X_train.shape} The amount of rows in Testing Df {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1340001-0490-44e9-8740-a80b40bdf9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spn                          0\n",
       "fmi                          0\n",
       "active                       0\n",
       "activeTransitionCount        0\n",
       "EquipmentID                  0\n",
       "BarometricPressure           0\n",
       "CruiseControlActive          0\n",
       "EngineCoolantTemperature     0\n",
       "EngineOilPressure            0\n",
       "EngineOilTemperature         0\n",
       "EngineRpm                    0\n",
       "FuelLtd                      0\n",
       "FuelRate                     0\n",
       "IgnStatus                    0\n",
       "IntakeManifoldTemperature    0\n",
       "ParkingBrake                 0\n",
       "Speed                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "027374d9-3085-4269-84f9-2a21dcffd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped_rows = df.merge(X_train, how=\"left\", indicator=True).query('_merge == \"left_only\"').drop(\"_merge\", axis=1)\n",
    "# print(F\"The amount of rows dropped for train/test split: {dropped_rows.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41d0de36-2131-469a-9ac3-3ffab26de794",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['spn', 'fmi', 'EquipmentID'] \n",
    "bool_features = ['ParkingBrake', 'IgnStatus', 'active']\n",
    "numeric_features = [x for x in X_train.columns if x not in categorical_features + bool_features] \n",
    "#pca = PCA(n_components=1, svd_solver=\"arpack\")\n",
    "\n",
    "numeric_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('scale', StandardScaler()),\n",
    "        ('numeric_impute', SimpleImputer(strategy='most_frequent'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True)),\n",
    "        ('categorical_impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "bool_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('bool_impute', SimpleImputer(strategy='most_frequent'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_pipe, numeric_features),\n",
    "        ('categorical', categorical_pipe, categorical_features),\n",
    "        ('bool', bool_pipe, bool_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('transformer', ct),\n",
    "        #('pca', pca)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "X_train_transformed = pipe.transform(X_train)\n",
    "X_test_transformed = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f7531-0eb7-4f25-bf29-15d9b4847394",
   "metadata": {},
   "source": [
    "### Creating Pickle File for Storing Pipe Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b7f9aaa-770d-403c-a206-a68742505997",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pipe_transformed.pkl'\n",
    "\n",
    "pickle_list = [pipe, X_train_transformed, X_test_transformed]\n",
    "\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(pickle_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "371cd373-fe65-463a-a912-6850d62496c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'rb') as file:\n",
    "    pipe, X_train_transformed, X_test_transformed = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6613e-1a59-4afe-8b5c-6e2179fbff57",
   "metadata": {},
   "source": [
    "### Defining Model Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d0ceb-e081-4476-8cac-fd12e7c193df",
   "metadata": {},
   "source": [
    "### Smoting To Solve Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16bc07f5-207c-4ed2-a507-d67964ead2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_trained_smoted, y_trained_smoted = smote.fit_resample(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7ac25-d652-4875-b5d0-b8dff145b75a",
   "metadata": {},
   "source": [
    "### LGBMClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc791b1e-49ac-4e62-b807-1bda7d44ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 324839, number of negative: 324839\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.491615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 68379\n",
      "[LightGBM] [Info] Number of data points in the train set: 649678, number of used features: 872\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[[20161   150]\n",
      " [ 1045    16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20311\n",
      "        True       0.10      0.02      0.03      1061\n",
      "\n",
      "    accuracy                           0.94     21372\n",
      "   macro avg       0.52      0.50      0.50     21372\n",
      "weighted avg       0.91      0.94      0.92     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LGBMClassiferModel = LGBMClassifier()\n",
    "LGBMClassiferModel = LGBMClassiferModel.fit(X_trained_smoted, y_trained_smoted)\n",
    "LGBMClassifer_y_pred = LGBMClassiferModel.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, LGBMClassifer_y_pred))\n",
    "print(classification_report(y_test, LGBMClassifer_y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd476e-ca7f-488c-b459-835cb53f6a7a",
   "metadata": {},
   "source": [
    "#### LGBM Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc4f8dba-7d7c-45da-8c5c-88bc9b383467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=10. Current value: lambda_l2=10\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=10. Current value: lambda_l2=10\n",
      "[LightGBM] [Info] Number of positive: 324839, number of negative: 324839\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.483596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 68379\n",
      "[LightGBM] [Info] Number of data points in the train set: 649678, number of used features: 872\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters: OrderedDict({'alpha': 0.5, 'colsample_bytree': 0.6, 'lambda': 10, 'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.6})\n",
      "Best score: 0.9540289048145064\n"
     ]
    }
   ],
   "source": [
    "param_grid = param_ranges = {\n",
    "    \"max_depth\": [3, 6, 9],  # Controls tree complexity\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],  # Step size for weight updates\n",
    "    \"subsample\": [0.6, 0.8, 1.0],  # Fraction of samples used per boosting iteration\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],  # Fraction of features used per tree\n",
    "    \"lambda\": [0.1, 1, 10],  # L2 regularization\n",
    "    \"alpha\": [0.1, 0.5, 1],  # L1 regularization\n",
    "    \"n_estimators\": [100, 500, 1000]  # Number of boosting rounds\n",
    "}\n",
    "bayes_search = BayesSearchCV(estimator=LGBMClassiferModel, search_spaces=param_grid, n_iter=25, cv=3, n_jobs=-1, verbose=2, scoring=\"f1\")\n",
    "bayes_search.fit(X_trained_smoted, y_trained_smoted)\n",
    "\n",
    "best_params = bayes_search.best_params_\n",
    "print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "print(f\"Best score: {bayes_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25818202-7cbd-40f9-8b11-323e675bd466",
   "metadata": {},
   "source": [
    "#### LGBM Model Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d84760d-f5a6-4171-8256-f1c7e423b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=10. Current value: lambda_l2=10\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=10. Current value: lambda_l2=10\n",
      "[LightGBM] [Info] Number of positive: 324839, number of negative: 324839\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.673343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 68379\n",
      "[LightGBM] [Info] Number of data points in the train set: 649678, number of used features: 872\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=10. Current value: lambda_l2=10\n",
      "[[20297    14]\n",
      " [ 1059     2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97     20311\n",
      "        True       0.12      0.00      0.00      1061\n",
      "\n",
      "    accuracy                           0.95     21372\n",
      "   macro avg       0.54      0.50      0.49     21372\n",
      "weighted avg       0.91      0.95      0.93     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LGBMClassifierTuned = LGBMClassifier(**best_params)\n",
    "LGBMClassifierTuned.fit(X_trained_smoted, y_trained_smoted)\n",
    "LGBMClassiferTuned_y_pred = LGBMClassifierTuned.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, LGBMClassiferTuned_y_pred))\n",
    "print(classification_report(y_test, LGBMClassiferTuned_y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63c51fa0-772a-4745-a312-6cd81d46f9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBMClassiferTuned_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848abbe-38f8-4da0-af09-2caa61f12e97",
   "metadata": {},
   "source": [
    "### XGBClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcfebf80-9f7d-4dac-a853-5c8fccf71a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20091   220]\n",
      " [ 1035    26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20311\n",
      "        True       0.11      0.02      0.04      1061\n",
      "\n",
      "    accuracy                           0.94     21372\n",
      "   macro avg       0.53      0.51      0.50     21372\n",
      "weighted avg       0.91      0.94      0.92     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBClassiferModel = XGBClassifier()\n",
    "XGBClassiferModel.fit(X_trained_smoted, y_trained_smoted)\n",
    "XGBClassifer_y_pred = XGBClassiferModel.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, XGBClassifer_y_pred))\n",
    "print(classification_report(y_test, XGBClassifer_y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a67e4c5-a75e-4bb9-9d01-263ae46973bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifer_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76160420-cf7b-46c5-a551-f68c489b2b3b",
   "metadata": {},
   "source": [
    "#### XGB Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f043b67-8ece-4abd-84f9-7364b2987e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters: OrderedDict({'alpha': 0.5, 'colsample_bytree': 0.6, 'lambda': 10, 'learning_rate': 0.3, 'max_depth': 9, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1.0})\n",
      "Best score: 0.9397046106895176\n"
     ]
    }
   ],
   "source": [
    "param_grid = param_ranges = {\n",
    "    \"max_depth\": [3, 6, 9],  # Controls tree complexity\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],  # Step size for weight updates\n",
    "    \"subsample\": [0.6, 0.8, 1.0],  # Fraction of samples used per boosting iteration\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],  # Fraction of features used per tree\n",
    "    \"min_child_weight\": [1, 3, 5],  # Minimum sum of instance weight in child node\n",
    "    \"lambda\": [0.1, 1, 10],  # L2 regularization\n",
    "    \"alpha\": [0, 0.5, 1],  # L1 regularization\n",
    "    \"n_estimators\": [100, 500, 1000]  # Number of boosting rounds\n",
    "}\n",
    "bayes_search = BayesSearchCV(estimator=XGBClassiferModel, search_spaces=param_grid, n_iter=25, cv=3, n_jobs=-1, verbose=2, scoring=\"f1\")\n",
    "bayes_search.fit(X_trained_smoted, y_trained_smoted)\n",
    "\n",
    "best_params = bayes_search.best_params_\n",
    "print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "print(f\"Best score: {bayes_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d8c3e-74ff-4fc7-a9d3-26b7b559f1f0",
   "metadata": {},
   "source": [
    "#### XGB Model Tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b6b64df-05cb-48ee-854d-0442439af186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20221    90]\n",
      " [ 1052     9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97     20311\n",
      "        True       0.09      0.01      0.02      1061\n",
      "\n",
      "    accuracy                           0.95     21372\n",
      "   macro avg       0.52      0.50      0.49     21372\n",
      "weighted avg       0.91      0.95      0.93     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBClassifierTuned = XGBClassifier(**best_params)\n",
    "XGBClassifierTuned.fit(X_trained_smoted, y_trained_smoted)\n",
    "XGBClassifierTuned_y_pred = XGBClassifierTuned.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, XGBClassifierTuned_y_pred))\n",
    "print(classification_report(y_test, XGBClassifierTuned_y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f910b-5da9-406d-b03d-f98ed668ae47",
   "metadata": {},
   "source": [
    "### Decision Tree Model Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75e1bc30-6444-4e11-804a-26cfdb987a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19148  1163]\n",
      " [  949   112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.94      0.95     20311\n",
      "        True       0.09      0.11      0.10      1061\n",
      "\n",
      "    accuracy                           0.90     21372\n",
      "   macro avg       0.52      0.52      0.52     21372\n",
      "weighted avg       0.91      0.90      0.91     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DecisionTreeClassifierModel = DecisionTreeClassifier()\n",
    "DecisionTreeClassifierModel.fit(X_trained_smoted, y_trained_smoted)\n",
    "DecisionTreeClassifierModel_y_pred = DecisionTreeClassifierModel.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, DecisionTreeClassifierModel_y_pred))\n",
    "print(classification_report(y_test, DecisionTreeClassifierModel_y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfb474-462b-4c47-8150-787be9888a3a",
   "metadata": {},
   "source": [
    "#### Decision Tree Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07eee6a2-f5fc-4aee-84e4-99050ed76646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters: OrderedDict({'criterion': 'gini', 'max_depth': 9, 'max_features': 'sqrt', 'max_leaf_nodes': 50, 'min_samples_leaf': 3, 'min_samples_split': 5})\n",
      "Best score: 0.6199826301752877\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"max_depth\": [3, 6, 9],  \n",
    "    \"min_samples_split\": [2, 5, 10],  \n",
    "    \"min_samples_leaf\": [1, 3, 5],  \n",
    "    \"max_features\": [\"sqrt\", \"log2\"],  # Removed \"auto\"\n",
    "    \"criterion\": [\"gini\", \"entropy\"],  \n",
    "    \"max_leaf_nodes\": [None, 10, 50]\n",
    "}\n",
    "bayes_search = BayesSearchCV(estimator=DecisionTreeClassifierModel, search_spaces=param_grid, n_iter=25, cv=3, n_jobs=-1, verbose=2, scoring=\"f1\")\n",
    "bayes_search.fit(X_trained_smoted, y_trained_smoted)\n",
    "\n",
    "best_params = bayes_search.best_params_\n",
    "print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "print(f\"Best score: {bayes_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4502e8-972d-4807-bab9-0c5d1696c26b",
   "metadata": {},
   "source": [
    "#### DecisionTree Model Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a6bb86b-88c7-4063-b307-cbf46bb4e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17922  2389]\n",
      " [  892   169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.88      0.92     20311\n",
      "        True       0.07      0.16      0.09      1061\n",
      "\n",
      "    accuracy                           0.85     21372\n",
      "   macro avg       0.51      0.52      0.50     21372\n",
      "weighted avg       0.91      0.85      0.88     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DecisionTreeClassifierTuned = DecisionTreeClassifier(**best_params)\n",
    "DecisionTreeClassifierTuned.fit(X_trained_smoted, y_trained_smoted)\n",
    "DecisionTreeClassifierTuned_y_pred = DecisionTreeClassifierTuned.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, DecisionTreeClassifierTuned_y_pred))\n",
    "print(classification_report(y_test, DecisionTreeClassifierTuned_y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282a646-c908-4db0-950f-3a95bbfbfc97",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bfb4e55-c081-4a0a-bd42-1880d5185b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11834  8477]\n",
      " [  670   391]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.58      0.72     20311\n",
      "        True       0.04      0.37      0.08      1061\n",
      "\n",
      "    accuracy                           0.57     21372\n",
      "   macro avg       0.50      0.48      0.40     21372\n",
      "weighted avg       0.90      0.57      0.69     21372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\billy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "LogisticRegressionModel = LogisticRegression(max_iter=500)\n",
    "LogisticRegressionModel.fit(X_trained_smoted, y_trained_smoted)\n",
    "LogisticRegressionModel_y_pred = LogisticRegressionModel.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, LogisticRegressionModel_y_pred))\n",
    "print(classification_report(y_test, LogisticRegressionModel_y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49eb2fa-c2ec-4431-9e44-b33f1e4bba30",
   "metadata": {},
   "source": [
    "#### Logisitic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d2ec2c35-af19-4e46-a636-469f7c6c73fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\billy\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [0.001, 200, 'l2', 'liblinear'] before, using random point [0.001, 1000, 'l2', 'liblinear']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\billy\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [0.001, 500, 'l2', 'liblinear'] before, using random point [1, 1000, 'l2', 'liblinear']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters: OrderedDict({'C': 100, 'max_iter': 500, 'penalty': 'l2', 'solver': 'liblinear'})\n",
      "Best score: 0.6522604651756612\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],  # Type of regularization\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength (inverse of )\n",
    "    \"solver\": [\"liblinear\"],  # Optimizer for l1/l2 penalties\n",
    "    \"max_iter\": [100, 200, 500, 1000]  # Iterations for convergence\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(estimator=LogisticRegressionModel, search_spaces=param_grid, n_iter=25, cv=3, n_jobs=-1, verbose=2, scoring=\"f1\")\n",
    "bayes_search.fit(X_trained_smoted, y_trained_smoted)\n",
    "\n",
    "best_params = bayes_search.best_params_\n",
    "print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "print(f\"Best score: {bayes_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48c4c2fa-f657-4d3c-a0d3-ba6a4348be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12318  7993]\n",
      " [  742   319]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.61      0.74     20311\n",
      "        True       0.04      0.30      0.07      1061\n",
      "\n",
      "    accuracy                           0.59     21372\n",
      "   macro avg       0.49      0.45      0.40     21372\n",
      "weighted avg       0.90      0.59      0.70     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogisticRegressionModelTuned = LogisticRegression(**best_params)\n",
    "LogisticRegressionModelTuned.fit(X_trained_smoted, y_trained_smoted)\n",
    "LogisticRegressionModelTuned_y_pred = LogisticRegressionModelTuned.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, LogisticRegressionModelTuned_y_pred))\n",
    "print(classification_report(y_test, LogisticRegressionModelTuned_y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda38bb-4c2e-4e54-98bd-d6e7dae2091e",
   "metadata": {},
   "source": [
    "### KNN Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "332691d7-802d-4f78-84f9-b1fa03802b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11834  8477]\n",
      " [  670   391]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.58      0.72     20311\n",
      "        True       0.04      0.37      0.08      1061\n",
      "\n",
      "    accuracy                           0.57     21372\n",
      "   macro avg       0.50      0.48      0.40     21372\n",
      "weighted avg       0.90      0.57      0.69     21372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KNeighborsClassifierModel = KNeighborsClassifier()\n",
    "KNeighborsClassifierModel.fit(X_trained_smoted, y_trained_smoted)\n",
    "KNeighborsClassifierModel_y_pred = KNeighborsClassifierModel.predict(X_test_transformed)\n",
    "print(confusion_matrix(y_test, KNeighborsClassifierModel_y_pred))\n",
    "print(classification_report(y_test, KNeighborsClassifierModel_y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14345d44-3940-4805-984f-27978acab2ff",
   "metadata": {},
   "source": [
    "##### I've aborted the Hyperparameter tuning for KNN because it doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33f04c-50fe-4431-8d20-f816dc100a7a",
   "metadata": {},
   "source": [
    "#### KNN Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "94e425e5-6aa0-4dbb-9aac-c86170793a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"n_neighbors\": [3, 5, 7, 9, 11],  # Number of nearest neighbors\n",
    "#     \"weights\": [\"uniform\", \"distance\"],  # How neighbors influence prediction\n",
    "#     \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],  # Distance calculation method\n",
    "#     \"p\": [1, 2],  # Minkowski power parameter (1 = Manhattan, 2 = Euclidean)\n",
    "# }\n",
    "\n",
    "# bayes_search = BayesSearchCV(estimator=KNeighborsClassifierModel, search_spaces=param_grid, n_iter=25, cv=3, n_jobs=-1, verbose=2, scoring=\"f1\")\n",
    "# bayes_search.fit(X_trained_smoted, y_trained_smoted)\n",
    "\n",
    "# best_params = bayes_search.best_params_\n",
    "# print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "# print(f\"Best score: {bayes_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04b0b8-8461-43da-b032-835ae0a1833c",
   "metadata": {},
   "source": [
    "#### KNN Model tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4156413a-892e-4cfd-87f1-763f71b0dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNeighborsClassifierModelTuned = LogisticRegression(**best_params)\n",
    "# KNeighborsClassifierModelTuned.fit(X_trained_smoted, y_trained_smoted)\n",
    "# KNeighborsClassifierModelTuned_y_pred = KNeighborsClassifierModelTuned.predict(X_test_transformed)\n",
    "# print(confusion_matrix(y_test, KNeighborsClassifierModelTuned_y_pred))\n",
    "# print(classification_report(y_test, KNeighborsClassifierModelTuned_y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248c2e1-a496-43e4-ab72-3d0f534057fe",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ddc33b5f-10ef-403e-812d-95e533c1b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_pred(df, event, equipment, target, pred):\n",
    "    df = df[[event, equipment, target]].copy()\n",
    "    df['predict'] = pred\n",
    "    df['predict'] = df['predict'].replace({0: 'False', 1: 'True'})\n",
    "    df['combined'] = df[target].astype(str) + '_' + df['predict'].astype(str)\n",
    "    df[event] = pd.to_datetime(df[event])\n",
    "    df = df.sort_values(by=[equipment, event])\n",
    "    df['time_diff'] = df.groupby(equipment, observed=True)[event].diff().dt.total_seconds() / 3600\n",
    "    df['valid_group'] = (df['time_diff'].isna()) | (df['time_diff'] <= 2)\n",
    "    df['temp_group'] = (~df['valid_group']).cumsum()\n",
    "    result = df.groupby([equipment, 'temp_group'], observed=True)['combined'].value_counts().reset_index()\n",
    "    result = df.groupby([equipment, 'temp_group'], observed=True)['combined'].value_counts().unstack(fill_value=0).drop_duplicates()\n",
    "    result = result.rename(columns = {'False_False': 'true negative', 'False_True': 'false positive', 'True_False': 'false negative', 'True_True': 'true positive'})\n",
    "    \n",
    "    for index, row in result.iterrows():\n",
    "        if row['true positive'] >= 1:\n",
    "            result.loc[index, 'true positive'] = 1\n",
    "            result.loc[index, ~result.columns.isin(['true positive'])] = 0\n",
    "        elif row['true positive'] == 0 and row['false positive'] >= 1:\n",
    "            result.loc[index, 'false positive'] = 1\n",
    "            result.loc[index, ~result.columns.isin(['false positive', 'true positive'])] = 0\n",
    "        elif row['true positive'] == 0 and row['false positive'] == 0 and row['false negative'] >= 1:\n",
    "            result.loc[index, 'false negative'] = 1\n",
    "            result.loc[index, ~result.columns.isin(['false negative', 'false positive', 'true positive'])] = 0\n",
    "        else:\n",
    "            result.loc[index, 'true negative'] = 1\n",
    "    counts = (result.iloc[:,0].sum() * 0) - (result.iloc[:,1].sum() * 500) + (result.iloc[:,3].sum() * 4000)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "39cd19c6-8767-423a-b83c-fffd54c98c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LGBM Model saved: 13000\n",
      "The LGBM Model tuned saved: 4500\n",
      "The XGBoost Model  saved: 23500\n",
      "The XGBoost Model tuned saved: 7000\n",
      "The Decision Tree Model saved: 42000\n",
      "The Decision Tree Model Tuned saved: 52500\n",
      "The Logistic Regression Model saved: 128500\n",
      "The Logistic Regression Model Tuned saved: 97500\n",
      "The KNN Model saved: 128500\n"
     ]
    }
   ],
   "source": [
    "print(F\"The LGBM Model saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', LGBMClassifer_y_pred)}\")\n",
    "print(F\"The LGBM Model tuned saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', LGBMClassiferTuned_y_pred)}\")\n",
    "print(F\"The XGBoost Model  saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', XGBClassifer_y_pred)}\")\n",
    "print(F\"The XGBoost Model tuned saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', XGBClassifierTuned_y_pred)}\")\n",
    "print(F\"The Decision Tree Model saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', DecisionTreeClassifierModel_y_pred)}\")\n",
    "print(F\"The Decision Tree Model Tuned saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', DecisionTreeClassifierTuned_y_pred)}\")\n",
    "print(F\"The Logistic Regression Model saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', LogisticRegressionModel_y_pred)}\")\n",
    "print(F\"The Logistic Regression Model Tuned saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', LogisticRegressionModelTuned_y_pred)}\")\n",
    "print(F\"The KNN Model saved: {target_pred(df_test, 'EventTimeStamp', 'EquipmentID', 'target', KNeighborsClassifierModel_y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef59b74-e92a-4c84-8491-91f912322af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
